{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d75fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ma-user/work/Yolov5_for_MindSpore_1.1_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d73e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "import mindspore as ms\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.nn.optim.momentum import Momentum\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.communication.management import init, get_rank, get_group_size\n",
    "from mindspore.train.callback import ModelCheckpoint, RunContext\n",
    "from mindspore.train.callback import _InternalCallbackParam, CheckpointConfig\n",
    "\n",
    "from src.yolo import YOLOV5s, YoloWithLossCell, TrainingWrapper\n",
    "from src.logger import get_logger\n",
    "from src.util import AverageMeter, get_param_groups\n",
    "from src.lr_scheduler import get_lr\n",
    "from src.yolo_dataset import create_yolo_dataset\n",
    "from src.initializer import default_recurisive_init, load_yolov5_params\n",
    "from src.config import ConfigYOLOV5\n",
    "ms.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b900ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(cloud_args=None):\n",
    "    \"\"\"Parse train arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser('mindspore coco training')\n",
    "\n",
    "    # device related\n",
    "    parser.add_argument('--device_target', type=str, default='Ascend',\n",
    "                        help='device where the code will be implemented.')\n",
    "\n",
    "    # dataset related\n",
    "    parser.add_argument('--data_dir', default='antigen', type=str, help='Train dataset directory.')\n",
    "    parser.add_argument('--per_batch_size', default=32, type=int, help='Batch size for Training. Default: 8')\n",
    "\n",
    "    # network related\n",
    "    parser.add_argument('--pretrained_backbone', default='', type=str,\n",
    "                        help='The backbone file of YOLOv5. Default: \"\".')\n",
    "    parser.add_argument('--resume_yolov5', default='', type=str,\n",
    "                        help='The ckpt file of YOLOv5, which used to fine tune. Default: \"\"')\n",
    "\n",
    "    # optimizer and lr related\n",
    "    parser.add_argument('--lr_scheduler', default='cosine_annealing', type=str,\n",
    "                        help='Learning rate scheduler, options: exponential, cosine_annealing. Default: exponential')\n",
    "    parser.add_argument('--lr', default=0.01, type=float, help='Learning rate. Default: 0.01')\n",
    "    parser.add_argument('--lr_epochs', type=str, default='220,250',\n",
    "                        help='Epoch of changing of lr changing, split with \",\". Default: 220,250')\n",
    "    parser.add_argument('--lr_gamma', type=float, default=0.1,\n",
    "                        help='Decrease lr by a factor of exponential lr_scheduler. Default: 0.1')\n",
    "    parser.add_argument('--eta_min', type=float, default=0., help='Eta_min in cosine_annealing scheduler. Default: 0')\n",
    "    parser.add_argument('--T_max', type=int, default=300, help='T-max in cosine_annealing scheduler. Default: 320')\n",
    "    parser.add_argument('--max_epoch', type=int, default=300, help='Max epoch num to train the model. Default: 320')\n",
    "    parser.add_argument('--warmup_epochs', default=1, type=float, help='Warmup epochs. Default: 0')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0005, help='Weight decay factor. Default: 0.0005')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='Momentum. Default: 0.9')\n",
    "\n",
    "    # loss related\n",
    "    parser.add_argument('--loss_scale', type=int, default=1024, help='Static loss scale. Default: 1024')\n",
    "    parser.add_argument('--label_smooth', type=int, default=0, help='Whether to use label smooth in CE. Default:0')\n",
    "    parser.add_argument('--label_smooth_factor', type=float, default=0.1,\n",
    "                        help='Smooth strength of original one-hot. Default: 0.1')\n",
    "\n",
    "    # logging related\n",
    "    parser.add_argument('--log_interval', type=int, default=100, help='Logging interval steps. Default: 100')\n",
    "    parser.add_argument('--ckpt_path', type=str, default='outputs/', help='Checkpoint save location. Default: outputs/')\n",
    "    parser.add_argument('--ckpt_interval', type=int, default=100, help='Save checkpoint interval. Default: 10')\n",
    "\n",
    "    parser.add_argument('--is_save_on_master', type=int, default=1,\n",
    "                        help='Save ckpt on master or all rank, 1 for master, 0 for all ranks. Default: 1')\n",
    "\n",
    "    # distributed related\n",
    "    parser.add_argument('--is_distributed', type=int, default=1,\n",
    "                        help='Distribute train or not, 1 for yes, 0 for no. Default: 1')\n",
    "    parser.add_argument('--rank', type=int, default=0, help='Local rank of distributed. Default: 0')\n",
    "    parser.add_argument('--group_size', type=int, default=1, help='World size of device. Default: 1')\n",
    "\n",
    "    # roma obs\n",
    "    parser.add_argument('--train_url', type=str, default=\"\", help='train url')\n",
    "    # profiler init\n",
    "    parser.add_argument('--need_profiler', type=int, default=0,\n",
    "                        help='Whether use profiler. 0 for no, 1 for yes. Default: 0')\n",
    "\n",
    "    # reset default config\n",
    "    parser.add_argument('--training_shape', type=str, default=\"\", help='Fix training shape. Default: \"\"')\n",
    "    parser.add_argument('--resize_rate', type=int, default=10,\n",
    "                        help='Resize rate for multi-scale training. Default: None')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    args = merge_args(args, cloud_args)\n",
    "    if args.lr_scheduler == 'cosine_annealing' and args.max_epoch > args.T_max:\n",
    "        args.T_max = args.max_epoch\n",
    "\n",
    "    args.lr_epochs = list(map(int, args.lr_epochs.split(',')))\n",
    "    args.data_root = os.path.join(args.data_dir, 'train')\n",
    "    args.annFile = os.path.join(args.data_dir, 'annotations/train.json')\n",
    "\n",
    "    devid = int(os.getenv('DEVICE_ID', '0'))\n",
    "    context.set_context(mode=context.GRAPH_MODE, enable_auto_mixed_precision=True,\n",
    "                        device_target=args.device_target, save_graphs=False)#, device_id=devid)\n",
    "    # init distributed\n",
    "    if args.is_distributed:\n",
    "        if args.device_target == \"Ascend\":\n",
    "            init()\n",
    "        else:\n",
    "            init(\"nccl\")\n",
    "        args.rank = get_rank()\n",
    "        args.group_size = get_group_size()\n",
    "\n",
    "    # select for master rank save ckpt or all rank save, compatible for model parallel\n",
    "    args.rank_save_ckpt_flag = 0\n",
    "    if args.is_save_on_master:\n",
    "        if args.rank == 0:\n",
    "            args.rank_save_ckpt_flag = 1\n",
    "    else:\n",
    "        args.rank_save_ckpt_flag = 1\n",
    "\n",
    "    # logger\n",
    "    args.outputs_dir = os.path.join(args.ckpt_path,\n",
    "                                    datetime.datetime.now().strftime('%Y-%m-%d_time_%H_%M_%S'))\n",
    "    args.logger = get_logger(args.outputs_dir, args.rank)\n",
    "    args.logger.save_args(args)\n",
    "\n",
    "    return args\n",
    "\n",
    "def merge_args(args, cloud_args):\n",
    "    args_dict = vars(args)\n",
    "    if isinstance(cloud_args, dict):\n",
    "        for key in cloud_args.keys():\n",
    "            val = cloud_args[key]\n",
    "            if key in args_dict and val:\n",
    "                arg_type = type(args_dict[key])\n",
    "                if arg_type is not type(None):\n",
    "                    val = arg_type(val)\n",
    "                args_dict[key] = val\n",
    "    return args\n",
    "\n",
    "\n",
    "def convert_training_shape(args_training_shape):\n",
    "    training_shape = [int(args_training_shape), int(args_training_shape)]\n",
    "    return training_shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31abd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(666:281473568998864,MainProcess):2022-09-19-17:28:56.114.170 [mindspore/context.py:768]  'enable_auto_mixed_precision' parameters will be deprecated.For details, please see the interface parameter API comments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-19 17:28:56,196:INFO:Args:\n",
      "2022-09-19 17:28:56,214:INFO:--> device_target: Ascend\n",
      "2022-09-19 17:28:56,216:INFO:--> data_dir: antigen\n",
      "2022-09-19 17:28:56,221:INFO:--> per_batch_size: 32\n",
      "2022-09-19 17:28:56,263:INFO:--> pretrained_backbone: \n",
      "2022-09-19 17:28:56,277:INFO:--> resume_yolov5: \n",
      "2022-09-19 17:28:56,282:INFO:--> lr_scheduler: cosine_annealing\n",
      "2022-09-19 17:28:56,289:INFO:--> lr: 0.01\n",
      "2022-09-19 17:28:56,298:INFO:--> lr_epochs: [220, 250]\n",
      "2022-09-19 17:28:56,308:INFO:--> lr_gamma: 0.1\n",
      "2022-09-19 17:28:56,314:INFO:--> eta_min: 0.0\n",
      "2022-09-19 17:28:56,351:INFO:--> T_max: 300\n",
      "2022-09-19 17:28:56,373:INFO:--> max_epoch: 300\n",
      "2022-09-19 17:28:56,379:INFO:--> warmup_epochs: 1\n",
      "2022-09-19 17:28:56,381:INFO:--> weight_decay: 0.0005\n",
      "2022-09-19 17:28:56,421:INFO:--> momentum: 0.9\n",
      "2022-09-19 17:28:56,436:INFO:--> loss_scale: 1024\n",
      "2022-09-19 17:28:56,448:INFO:--> label_smooth: 0\n",
      "2022-09-19 17:28:56,485:INFO:--> label_smooth_factor: 0.1\n",
      "2022-09-19 17:28:56,505:INFO:--> log_interval: 100\n",
      "2022-09-19 17:28:56,520:INFO:--> ckpt_path: outputs/\n",
      "2022-09-19 17:28:56,526:INFO:--> ckpt_interval: 100\n",
      "2022-09-19 17:28:56,561:INFO:--> is_save_on_master: 1\n",
      "2022-09-19 17:28:56,584:INFO:--> is_distributed: 1\n",
      "2022-09-19 17:28:56,593:INFO:--> rank: 0\n",
      "2022-09-19 17:28:56,598:INFO:--> group_size: 1\n",
      "2022-09-19 17:28:56,604:INFO:--> train_url: \n",
      "2022-09-19 17:28:56,632:INFO:--> need_profiler: 0\n",
      "2022-09-19 17:28:56,655:INFO:--> training_shape: \n",
      "2022-09-19 17:28:56,659:INFO:--> resize_rate: 10\n",
      "2022-09-19 17:28:56,661:INFO:--> data_root: antigen/train\n",
      "2022-09-19 17:28:56,662:INFO:--> annFile: antigen/annotations/train.json\n",
      "2022-09-19 17:28:56,669:INFO:--> rank_save_ckpt_flag: 1\n",
      "2022-09-19 17:28:56,695:INFO:--> outputs_dir: outputs/2022-09-19_time_17_28_56\n",
      "2022-09-19 17:28:56,717:INFO:--> logger: <LOGGER YOLOV5 (NOTSET)>\n",
      "2022-09-19 17:28:56,730:INFO:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(666:281473568998864,MainProcess):2022-09-19-17:29:00.993.321 [mindspore/dataset/core/config.py:464] The shared memory is on, multiprocessing performance will be improved. Note: the required shared memory can't exceeds 80% of the available shared memory. You can reduce max_rowsize or reduce num_parallel_workers to reduce shared memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "2022-09-19 17:29:06,030:INFO:Finish loading dataset\n"
     ]
    }
   ],
   "source": [
    "cloud_args=None\n",
    "args = parse_args(cloud_args)\n",
    "loss_meter = AverageMeter('loss')\n",
    "\n",
    "context.reset_auto_parallel_context()\n",
    "parallel_mode = ParallelMode.STAND_ALONE\n",
    "degree = 1\n",
    "if args.is_distributed:\n",
    "    parallel_mode = ParallelMode.DATA_PARALLEL\n",
    "    degree = get_group_size()\n",
    "context.set_auto_parallel_context(parallel_mode=parallel_mode, gradients_mean=True, device_num=degree)\n",
    "\n",
    "network = YOLOV5s(is_training=True)\n",
    "# default is kaiming-normal\n",
    "default_recurisive_init(network)\n",
    "load_yolov5_params(args, network)\n",
    "\n",
    "network = YoloWithLossCell(network)\n",
    "config = ConfigYOLOV5()\n",
    "\n",
    "config.label_smooth = args.label_smooth\n",
    "config.label_smooth_factor = args.label_smooth_factor\n",
    "\n",
    "if args.training_shape:\n",
    "    config.multi_scale = [convert_training_shape(args.training_shape)]\n",
    "if args.resize_rate:\n",
    "    config.resize_rate = args.resize_rate\n",
    "\n",
    "ds, data_size = create_yolo_dataset(image_dir=args.data_root, anno_path=args.annFile, is_training=True,\n",
    "                                    batch_size=args.per_batch_size, max_epoch=args.max_epoch,\n",
    "                                    device_num=args.group_size, rank=args.rank, config=config)\n",
    "args.logger.info('Finish loading dataset')\n",
    "\n",
    "args.steps_per_epoch = int(data_size / args.per_batch_size / args.group_size)\n",
    "\n",
    "if not args.ckpt_interval:\n",
    "    args.ckpt_interval = args.steps_per_epoch\n",
    "\n",
    "lr = get_lr(args)\n",
    "\n",
    "opt = Momentum(params=get_param_groups(network),\n",
    "               learning_rate=Tensor(lr),\n",
    "               momentum=args.momentum,\n",
    "               weight_decay=args.weight_decay,\n",
    "               loss_scale=args.loss_scale)\n",
    "\n",
    "network = TrainingWrapper(network, opt, args.loss_scale // 2)\n",
    "network.set_train()\n",
    "\n",
    "if args.rank_save_ckpt_flag:\n",
    "    # checkpoint save\n",
    "    ckpt_max_num = args.max_epoch * args.steps_per_epoch // args.ckpt_interval\n",
    "    ckpt_config = CheckpointConfig(save_checkpoint_steps=args.ckpt_interval,\n",
    "                                   keep_checkpoint_max=ckpt_max_num)\n",
    "    save_ckpt_path = os.path.join(args.outputs_dir, 'ckpt_' + str(args.rank) + '/')\n",
    "    ckpt_cb = ModelCheckpoint(config=ckpt_config,\n",
    "                              directory=save_ckpt_path,\n",
    "                              prefix='{}'.format(args.rank))\n",
    "    cb_params = _InternalCallbackParam()\n",
    "    cb_params.train_network = network\n",
    "    cb_params.epoch_num = ckpt_max_num\n",
    "    cb_params.cur_epoch_num = 1\n",
    "    run_context = RunContext(cb_params)\n",
    "    ckpt_cb.begin(run_context)\n",
    "\n",
    "old_progress = -1\n",
    "t_end = time.time()\n",
    "data_loader = ds.create_dict_iterator(output_numpy=True, num_epochs=1)\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    images = data[\"image\"]\n",
    "    input_shape = images.shape[2:4]\n",
    "    images = Tensor.from_numpy(images)\n",
    "    batch_y_true_0 = Tensor.from_numpy(data['bbox1'])\n",
    "    batch_y_true_1 = Tensor.from_numpy(data['bbox2'])\n",
    "    batch_y_true_2 = Tensor.from_numpy(data['bbox3'])\n",
    "    batch_gt_box0 = Tensor.from_numpy(data['gt_box1'])\n",
    "    batch_gt_box1 = Tensor.from_numpy(data['gt_box2'])\n",
    "    batch_gt_box2 = Tensor.from_numpy(data['gt_box3'])\n",
    "    input_shape = Tensor(tuple(input_shape[::-1]), ms.float32)\n",
    "    loss = network(images, batch_y_true_0, batch_y_true_1, batch_y_true_2, batch_gt_box0, batch_gt_box1,\n",
    "                   batch_gt_box2, input_shape)\n",
    "    loss_meter.update(loss.asnumpy())\n",
    "\n",
    "    if args.rank_save_ckpt_flag:\n",
    "        # ckpt progress\n",
    "        cb_params.cur_step_num = i + 1  # current step number\n",
    "        cb_params.batch_num = i + 2\n",
    "        ckpt_cb.step_end(run_context)\n",
    "\n",
    "    if i % args.log_interval == 0:\n",
    "        time_used = time.time() - t_end\n",
    "        epoch = int(i / args.steps_per_epoch)\n",
    "        fps = args.per_batch_size * (i - old_progress) * args.group_size / time_used\n",
    "        if args.rank == 0:\n",
    "            args.logger.info(\n",
    "                'epoch[{}], iter[{}], {}, fps:{:.2f} imgs/sec, lr:{}'.format(epoch, i, loss_meter, fps, lr[i]))\n",
    "        t_end = time.time()\n",
    "        loss_meter.reset()\n",
    "        old_progress = i\n",
    "\n",
    "    if (i + 1) % args.steps_per_epoch == 0 and args.rank_save_ckpt_flag:\n",
    "        cb_params.cur_epoch_num += 1\n",
    "\n",
    "args.logger.info('==========end training===============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b650a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
